{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 11 - Deep learning intro: Multilayer perceptron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin, make sure you have updated your venv with `tutorial11.yml`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MNIST is a widely used for classification of images. The dataset is composed of 60,000 28x28 grayscale images of the 10 digits, along with a test set of 10,000 images. It is a subset of a larger set available from NIST. More information can be found at the [MNIST homepage](http://yann.lecun.com/exdb/mnist/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main ML topic:\n",
    "Supervised learning. We will introduce the basics of deep learning and show the capability of *Multilayer perceptron (MLP)*. This architecture is a type of a large field of neural networks that are calles *feed-forward neural networks*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our mission\n",
    "This tutorial's mission is to train a neural network that would identify your ID hand-written digits. For this taks I ask you to Write down your ID number on a white paper using a dark pen (blue or black). The digits should be separated from each other and about the size of 2X2 cm. Take a picture of your ID number from a distance of about 0.5m with your default focus. Make sure your image is clear and illuminated. Upload the image to your computer and cut each and every digit using \"snipping tool\". Make sure your digits are saved in PNG format. Name your first digit \"1.PNG\" and the second \"2.PNG\" etc. Save your 9 images in the same folder with your notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory reminders\n",
    "The pereceptron is a model we have already seen before in the name of *logistic regression*. The main limitation of this model is that it is **linear**. Thus, mostly, it would fail to classify some real-life data since the relations between the label and the data are highly non-linear. There is even one famous task that is extreamly simple but fails this model. It is known as the \"*XOR prbolem*\". \n",
    "\n",
    "\n",
    "However, the perceptron itself is really similar to a neuron as shown below:\n",
    "<center><img src=\"Images/1.PNG\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The connctivity with other many neurons and the combination of non-linear activation function between them were assumed to be the key for solving complicated tasks as shown here:\n",
    "<center><img src=\"Images/mlp.png\"><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An artificial neural network that its' connections between the nodes do not form a cycle, is called *feedforward* neural network. If, in addition to that, all of the neurons are connected to each other and weights are not shared, it is called *densed* or *fully-connected* neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to non-linear activation functions, the network cannot be explictly written as matrix multiplication and it is also makes the loss function highly non-convex. Thus, numeric solutions have to be applied such as *gradient descent*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The calculated gradients are still the loss function w.r.t every weight, and in order to calculate them through the layers we use the *chain rule*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The high number of examples within our training set, mostly will not allow us to save all of it in the computer's memory. Thus, we divide our training sets into *batches*. Due to linearity of the gradient operator, we can first accumulate the loss for all of the examples in the batch and only then calculate the gradient for updating the weights.\n",
    "\n",
    "For proper training, we should \"go through\" all of our training sets several times in what we call \"*epochs*\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define some terms:\n",
    "* **Epoch**: A state where our model has \"seen\" all of our training set.\n",
    "* **batch**: A subset of the training set to accumulate the loss for. The gradient is calculated and ehe weights are updated only after accumulating (summing) all of the loss terms in the batch.\n",
    "* **batch size**: number of examples within a batch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "import sys\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "mpl.style.use(['ggplot']) \n",
    "%matplotlib inline\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from IPython.display import display, clear_output\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will import the packages of `tensorflow` and `keras` which are widely used as Python's platforms for deep learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with checking whether or not an accessible GPU exists on our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if tf.config.list_physical_devices('GPU'):\n",
    "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
    "else:\n",
    "    print(\"Either GPU does not exist or simply not accessible (cuda is not installed).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also check if cuda is inastalled by the url `chrome://gpu/` in your Chrome browser. Use `Ctrl+F` for `cuda`. If `GPU CUDA compute capability major version` is 0 then cuda is not installed on your GPU.\n",
    "\n",
    "If a `TensorFlow` operation has both CPU and GPU implementations, by default the GPU devices will be given priority when the operation is assigned to a device."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like a particular operation to run on a device of your choice instead of what's automatically selected for you, you can use `with tf.device` to create a device context, and all the operations within that context will run on the same designated device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Place tensors on the CPU\n",
    "with tf.device('/CPU:0'):\n",
    "    a = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "    b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])\n",
    "\n",
    "# Run on the GPU (if exists)\n",
    "c = tf.matmul(a, b)\n",
    "print(c)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's load MNIST from Keras datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "data  = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specific task:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_orig, Y_train_orig), (x_test, y_test) = mnist.load_data() #60,000 for training, 10000 for testing\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train_orig, Y_train_orig, stratify=Y_train_orig, test_size=0.15, random_state=336546)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we will have a look on MNIST dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 3, figsize=(8,8))\n",
    "for i, ax in enumerate(axes.flatten()): \n",
    "    ax.imshow(x_train[i], cmap='gray', interpolation='none')\n",
    "    ax.set_title(\"Digit: {}\".format(y_train[i]))\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shapes of all of the datasest (x,y of training, validation and testing). Make sure that the shapes of the data make sense to you"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C_1\n",
    "#------------------------------Implement your code here------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape one of the training examples as a 1-rank array and plot its histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C2\n",
    "#------------------------------Implement your code here------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now reshape all of the X data (including `X_train_orig`) in the shape of `(num of examples, 784)`. Then convert it into `float32` using `np.astype()` and then nromalize it by 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C3\n",
    "#------------------------------Implement your code here------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram of the same example that you chose to make sure you applied normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C5\n",
    "#------------------------------Implement your code here------------------------\n",
    "\n",
    "#------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we should encode our labels as one-hot vectors. We can do it easily by using `utils` of `keras`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding using keras' numpy-related utilities\n",
    "n_classes = 10\n",
    "print(\"Shape before one-hot encoding: \", y_train.shape)\n",
    "Y_train_orig = utils.to_categorical(Y_train_orig, n_classes)\n",
    "Y_train = utils.to_categorical(y_train, n_classes)\n",
    "Y_val = utils.to_categorical(y_val, n_classes)\n",
    "Y_test = utils.to_categorical(y_test, n_classes)\n",
    "print(\"Shape after one-hot encoding: \", Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we got what we expected to have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train[0])\n",
    "print(Y_train[0,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can build our first neural netwrok! \n",
    "\n",
    "Let's see how can we build a simple, but powerful, neural network composed of two hidden layers with *Relu* activation and 512 neurons in each layer. Since it is a feedforward network, we should use `Sequential` class. Our output should be activated with *softmax* since it is a multiclass task and not *multilabel* for instance. For regularization, we would also add *dropout* with probaility of 0.2.\n",
    "\n",
    "It is useful to name our models and some of their activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = Sequential(name=\"MLP_1\")\n",
    "model_1.add(Dense(512, input_shape=(784,)))\n",
    "model_1.add(Activation('relu', name='Relu_1'))                            \n",
    "model_1.add(Dropout(0.2))\n",
    "\n",
    "model_1.add(Dense(512))\n",
    "model_1.add(Activation('relu', name='Relu_2'))\n",
    "model_1.add(Dropout(0.2))\n",
    "\n",
    "model_1.add(Dense(10))\n",
    "model_1.add(Activation('softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is it, you have built your first neural network! Let's have a look on its' summery: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look how many parmeters are learned within this network. This can inform us how complex the model is comparing to other models we saw in the course and this is considered a very simple model.\n",
    "\n",
    "The next thing to do is to choose our loss function, the metrics that we would like to calculate during the iterations and which optimizer it should use. We do it by the method `compile`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model is now ready to train and has its own initialized weights. Since we use a validation set before the overall training, and because `keras` models **do not** re-initialize the weights by default, we should save these weights for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not(\"results\" in os.listdir()):\n",
    "    os.mkdir(\"results\")\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_1.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_1.save(model_path)\n",
    "print('Saved initialized model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can train our model using the regular `fit` method we are so familiar with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_1.fit(x_train, Y_train,\n",
    "          batch_size=128, epochs=20,\n",
    "          verbose=2,\n",
    "          validation_data=(x_val, Y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to check what has occured within the iterations, we can use `history` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can plot the Training loss and accuracy vs. the validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "axs[0].plot(history.history['accuracy'])\n",
    "axs[0].plot(history.history['val_accuracy'])\n",
    "axs[0].set_title('model accuracy')\n",
    "axs[0].set_ylabel('accuracy')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].legend(['train', 'val'], loc='lower right')\n",
    "\n",
    "\n",
    "axs[1].plot(history.history['loss'])\n",
    "axs[1].plot(history.history['val_loss'])\n",
    "axs[1].set_title('model loss')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: for much deeper insights and visualizations, you might want to have a look on [TensorBoard](https://www.tensorflow.org/tensorboard)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build another **simple** model that you think would work and save the initialized weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C_6\n",
    "model_2 = Sequential(name=\"MLP_2\")\n",
    "#------------------------------Implement your code here------------------------\n",
    "\n",
    "#---------------------------------------------------------------------------------\n",
    "model_2.add(Dense(10))\n",
    "model_2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_2.compile(loss='categorical_crossentropy', metrics=['accuracy'], optimizer='adam')\n",
    "\n",
    "save_dir = \"results/\"\n",
    "model_name = \"init_weigths_2.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_2.save(model_path)\n",
    "print('Saved initialized model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model. You may try different hyperparmeters such as batch size or number of epochs. Name the history of the model as `history_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#C7\n",
    "#------------------------------Implement your code here------------------------\n",
    "\n",
    "#---------------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(12,5))\n",
    "\n",
    "axs[0].plot(history_2.history['accuracy'])\n",
    "axs[0].plot(history_2.history['val_accuracy'])\n",
    "axs[0].set_title('model accuracy')\n",
    "axs[0].set_ylabel('accuracy')\n",
    "axs[0].set_xlabel('epoch')\n",
    "axs[0].legend(['train', 'val'], loc='lower right')\n",
    "\n",
    "\n",
    "axs[1].plot(history_2.history['loss'])\n",
    "axs[1].plot(history_2.history['val_loss'])\n",
    "axs[1].set_title('model loss')\n",
    "axs[1].set_ylabel('loss')\n",
    "axs[1].set_xlabel('epoch')\n",
    "axs[1].legend(['train', 'val'], loc='upper right')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should now train our chosen model (`model_1` by default) with the complete training set. Before we do it, we need to initialize our model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_1 = load_model(\"results/init_weigths_1.h5\") # Initializing weights before total run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_1.fit(X_train_orig, Y_train_orig,\n",
    "          batch_size=128, epochs=20,\n",
    "          verbose=2)\n",
    "# saving the model\n",
    "save_dir = \"results/\"\n",
    "model_name = \"final_weights.h5\"\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model_1.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can evaluate our model performances on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_model = load_model(\"results/final_weights.h5\")\n",
    "loss_and_metrics = mnist_model.evaluate(x_test, Y_test, verbose=2)\n",
    "\n",
    "print(\"Test Loss is {:.2f} \".format(loss_and_metrics[0]))\n",
    "print(\"Test Accuracy is {:.2f} %\".format(100*loss_and_metrics[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the most interesting part of this tutorial, we will see whehter or not our neural network can figure our hand written digits. In order to do so, we will use the `pillow` package and we would make some preprocessing such as converting RGB to grayscale, looking at the \"negative\", resizing the images and strach the contrast by a hard threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "orig = []\n",
    "fig, axes = plt.subplots(1, 9, figsize=(15,15))\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    image = Image.open(str(i+1) + '.PNG')\n",
    "    gray = image.convert('L')\n",
    "    new_image = gray.resize((28, 28))\n",
    "    data = np.asarray(new_image).astype('float32')\n",
    "    data = (255-data)\n",
    "    data /= 255\n",
    "    ax.imshow(data, cmap=plt.get_cmap('gray'), vmin=0, vmax=1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    orig.append(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(orig[8].reshape(784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 9, figsize=(15,15))\n",
    "thresh_img = []\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    temp = orig[i].copy()\n",
    "    temp[temp<np.percentile(temp, 85)]=0\n",
    "    temp[temp>np.percentile(temp, 90)] *=2\n",
    "    temp[temp>1] = 1\n",
    "    thresh_img.append(temp)\n",
    "    ax.imshow(temp, cmap=plt.get_cmap('gray'), vmin=0, vmax=1)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your ID should appear below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = \"\"\n",
    "for img in thresh_img:\n",
    "    img = img.reshape(1,28*28)\n",
    "    pred += str(np.argmax(mnist_model.predict(img), axis=-1).item())\n",
    "print(\"Your ID is: {}\".format(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(thresh_img[8].reshape(784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *This tutorial was written by [Moran Davoodi](mailto:morandavoodi@gmail.com) with the assitance of [Yuval Ben Sason](mailto:yuvalbse@gmail.com) & Kevin Kotzen*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
